import sounddevice as sd
import numpy as np
import soundfile as sf
import tempfile
import os
import subprocess
import time
import re
from difflib import SequenceMatcher

from faster_whisper import WhisperModel
from ollama import Client
from text_processor import process_ai_response, sanitize_user_input
from memory import ConversationMemory

# ================== CONFIG ==================
SAMPLE_RATE = 16000

WAKE_SECONDS   = 3
RECORD_SECONDS = 6

WAKE_WORD = "B·∫£o ∆°i"

OLLAMA_MODEL = "gemma3:1b"

BEEP_START = "assets/bip.wav"
BEEP_STOP  = "assets/bip2.wav"

PIPER_EXE  = r"piper.exe"
TTS_MODEL  = r"assets\vi_VN-vais1000-medium.onnx"
TTS_OUT    = r"assets\answer.wav"
# ===========================================


# ---------- Ollama ----------
client = Client(host="http://localhost:11434")

# ---------- Memory ----------
memory = ConversationMemory("conversation_memory.json", max_history=20)

# ---------- STT ----------
print("‚è≥ Loading Whisper STT model (Vietnamese)...")
stt_model = WhisperModel(
    "medium",          # t·ªët cho ti·∫øng Vi·ªát
    device="cuda",
    compute_type="float16"
)
print("‚úÖ STT loaded")


# ================== UTILS ==================
def remove_vietnamese_accents(text: str) -> str:
    text = text.lower()
    replacements = {
        "√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ": "a",
        "√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ": "e",
        "√¨√≠·ªã·ªâƒ©": "i",
        "√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°": "o",
        "√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ": "u",
        "·ª≥√Ω·ªµ·ª∑·ªπ": "y",
        "ƒë": "d",
    }
    for chars, rep in replacements.items():
        for c in chars:
            text = text.replace(c, rep)
    return text


def similarity(a, b):
    return SequenceMatcher(None, a, b).ratio()


# ================== AUDIO ==================
def play_sound(path):
    if not os.path.exists(path):
        return
    data, sr = sf.read(path, dtype="float32")
    sd.play(data, sr)
    sd.wait()


def record_audio(seconds):
    audio = sd.rec(
        int(seconds * SAMPLE_RATE),
        samplerate=SAMPLE_RATE,
        channels=1,
        dtype=np.float32
    )
    sd.wait()
    return audio.flatten()


# ================== STT ==================
def speech_to_text(audio):
    with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
        sf.write(f.name, audio, SAMPLE_RATE)
        wav_path = f.name

    segments, _ = stt_model.transcribe(wav_path, language="vi")
    os.remove(wav_path)

    text = "".join(seg.text for seg in segments).strip()
    return sanitize_user_input(text)


# ================== WAKE WORD ==================
def detect_wake_word(text: str) -> bool:
    if not text:
        return False

    raw = text.lower()
    norm = remove_vietnamese_accents(raw)

    # C√°ch 1: ch·ª©a t·ª´ kho√°
    if "bao" in norm and "oi" in norm:
        return True

    # C√°ch 2: fuzzy matching
    score = similarity(norm, remove_vietnamese_accents(WAKE_WORD))
    return score >= 0.4   # ch·ªânh 0.5‚Äì0.7 tu·ª≥ m√¥i tr∆∞·ªùng

# h√†m ch·ªù ƒë√°nh th·ª©c b·∫±ng wake word
def wait_for_wake_word():
    print(f"üü° Ch·ªù wake word: '{WAKE_WORD}' ...")
    audio = record_audio(WAKE_SECONDS)
    text = speech_to_text(audio)

    if text:
        print("üëÇ Nghe:", text)
        return detect_wake_word(text)

    return False


# ================== LLM ==================
def ask_ollama(text):
    memory.add_message("user", text)
    
    # System prompt v·ªõi rules
    system_prompt = """B·∫°n l√† B·∫£o, m·ªôt tr·ª£ l√Ω h·ªó tr·ª£ ƒë∆∞·ª£c t·∫°o ra b·ªüi H√†n B·∫£o.

RULES:
- T√™n c·ªßa b·∫°n l√† B·∫£o
- B·∫°n ƒë∆∞·ª£c t·∫°o ra b·ªüi H√†n B·∫£o
- CH·ªà tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT, kh√¥ng d√πng ng√¥n ng·ªØ kh√°c
- Lu√¥n th√¢n thi·ªán, h·ªó tr·ª£ ng∆∞·ªùi d√πng
- N·∫øu ƒë∆∞·ª£c h·ªèi b·∫±ng ti·∫øng kh√°c, h√£y tr·∫£ l·ªùi ti·∫øng Vi·ªát, kh√¥ng d√πng icons hay emoji. Vi·∫øt ƒëo·∫°n vƒÉn kh√¥ng c√≥ k√Ω hi·ªáu ƒë·∫∑t bi·ªát, g·∫°ch ƒë·∫ßu d√≤ng, hay ƒë·ªãnh d·∫°ng markdown."""
    
    # X√¢y d·ª±ng messages v·ªõi system prompt
    messages = [{"role": "system", "content": system_prompt}]
    messages.extend(memory.get_context())
    
    response = client.chat(
        model=OLLAMA_MODEL,
        messages=messages
    )

    answer = response["message"]["content"].strip()
    answer = process_ai_response(answer, max_chars=500)

    memory.add_message("assistant", answer)
    print("ü§ñ B·∫£o:", answer)
    return answer


# ================== TTS ==================
def text_to_speech(text):
    subprocess.run(
        [
            PIPER_EXE,
            "--model", TTS_MODEL,
            text,
            "--output-file", TTS_OUT
        ],
        check=True
    )
    data, sr = sf.read(TTS_OUT, dtype="float32")
    sd.play(data, sr)
    sd.wait()


# ================== MAIN ==================
def main_loop():
    print("\nüü¢ Voice assistant started (wake-word fuzzy mode)\n")

    while True:
        try:
            if not wait_for_wake_word():
                continue

            print("üü¢ Wake word detected!")
            play_sound(BEEP_START)

            audio = record_audio(RECORD_SECONDS)
            play_sound(BEEP_STOP)

            text = speech_to_text(audio)
            if not text:
                continue

            print("üìù User:", text)

            answer = ask_ollama(text)
            if answer:
                text_to_speech(answer)

            print("-" * 50)

        except KeyboardInterrupt:
            print("\nüõë Exit")
            break

        except Exception as e:
            print("‚ùå Error:", e)
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main_loop()
